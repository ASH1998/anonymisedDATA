{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mat\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "mat.rcParams['agg.path.chunksize'] = 1000000000000\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import sklearn.preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME = \"zipfilee_FILES/ds_data/\"\n",
    "TRAIN_FILE = \"data_train.csv\"\n",
    "TEST_FILE = \"data_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DIR_NAME+TRAIN_FILE)\n",
    "test = pd.read_csv(DIR_NAME+TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596000, 58)\n",
      "(892816, 57)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num18   107909\n",
      "unique count  5013\n",
      "----------------------------\n",
      "num19   5\n",
      "unique count  5\n",
      "----------------------------\n",
      "num20   1\n",
      "unique count  184\n",
      "----------------------------\n",
      "num22   42667\n",
      "unique count  850\n",
      "----------------------------\n",
      "cat1   217\n",
      "unique count  5\n",
      "----------------------------\n",
      "cat2   83\n",
      "unique count  3\n",
      "----------------------------\n",
      "cat3   5814\n",
      "unique count  8\n",
      "----------------------------\n",
      "cat4   107\n",
      "unique count  13\n",
      "----------------------------\n",
      "cat5   5\n",
      "unique count  3\n",
      "----------------------------\n",
      "cat6   411792\n",
      "unique count  3\n",
      "----------------------------\n",
      "cat8   266928\n",
      "unique count  3\n",
      "----------------------------\n",
      "cat10   11503\n",
      "unique count  3\n",
      "----------------------------\n",
      "cat12   570\n",
      "unique count  6\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in train.columns:\n",
    "    if train[i].isna().sum()>0:\n",
    "        print(i ,\" \" , train[i].isna().sum())\n",
    "        print( \"unique count \", len(train[i].unique()))\n",
    "        print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#too many unique values as well as too many missing values, let's remove it\n",
    "train.drop('num18', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#too many missing values, and less categories, lets make the missing as another category, here 88,99 and 66 for example.\n",
    "train.cat6.fillna(88, inplace=True)\n",
    "train.cat8.fillna(99, inplace=True)\n",
    "train.cat10.fillna(66, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat1  completed!\n",
      "cat2  completed!\n",
      "cat3  completed!\n",
      "cat4  completed!\n",
      "cat5  completed!\n",
      "cat12  completed!\n"
     ]
    }
   ],
   "source": [
    "#Replace these categories with the most frequent label.\n",
    "agg_cat = [\"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat12\"]\n",
    "for i in agg_cat:\n",
    "    max_ = train[i].value_counts()\n",
    "    for j in max_.index:\n",
    "        if max_[j] == max_.max():\n",
    "            val = j\n",
    "    train[i].fillna(j, inplace=True)\n",
    "    print(i, \" completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest less missing values\n",
    "train.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id  completed...\n",
      "num1  completed...\n",
      "num2  completed...\n",
      "num3  completed...\n",
      "num4  completed...\n",
      "num5  completed...\n",
      "num6  completed...\n",
      "num7  completed...\n",
      "num8  completed...\n",
      "num9  completed...\n",
      "num10  completed...\n",
      "num11  completed...\n",
      "num12  completed...\n",
      "num13  completed...\n",
      "num14  completed...\n",
      "num15  completed...\n",
      "num16  completed...\n",
      "num17  completed...\n",
      "num19  completed...\n",
      "num20  completed...\n",
      "num21  completed...\n",
      "num22  completed...\n",
      "num23  completed...\n",
      "der1  completed...\n",
      "der2  completed...\n",
      "der3  completed...\n",
      "der4  completed...\n",
      "der5  completed...\n",
      "der6  completed...\n",
      "der7  completed...\n",
      "der8  completed...\n",
      "der9  completed...\n",
      "der10  completed...\n",
      "der11  completed...\n",
      "der12  completed...\n",
      "der13  completed...\n",
      "der14  completed...\n",
      "der15  completed...\n",
      "der16  completed...\n",
      "der17  completed...\n",
      "der18  completed...\n",
      "der19  completed...\n",
      "cat1  completed...\n",
      "cat2  completed...\n",
      "cat3  completed...\n",
      "cat4  completed...\n",
      "cat5  completed...\n",
      "cat6  completed...\n",
      "cat7  completed...\n",
      "cat8  completed...\n",
      "cat9  completed...\n",
      "cat10  completed...\n",
      "cat11  completed...\n",
      "cat12  completed...\n",
      "cat13  completed...\n",
      "cat14  completed...\n",
      "target  completed...\n"
     ]
    }
   ],
   "source": [
    "train_df = train.copy()\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "for i in train_df.columns:\n",
    "    train_df[i] = le.fit_transform(train_df[i])\n",
    "    print(i, \" completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.target\n",
    "train_df.drop([\"target\", \"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536400, 55)  and  (536400,)\n",
      "(59600, 55)  and  (59600,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape , \" and \", y_train.shape)\n",
    "print(x_val.shape, \" and \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \n",
    "         \"Decision Tree\", \"Random Forest\", \"PerceptronCLF\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    " #\"Linear SVM\",\"RBF SVM\", \"Gaussian Process\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_job = -1\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_jobs=n_job),\n",
    "    #SVC(kernel=\"linear\", C=0.025, verbose=1),\n",
    "    #SVC(gamma=2, C=1, verbose=1),\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0), n_jobs=2),\n",
    "    DecisionTreeClassifier(max_depth=50,),\n",
    "    RandomForestClassifier(max_depth=50, n_estimators=10, n_jobs=n_job),\n",
    "    MLPClassifier(hidden_layer_sizes=(240,), verbose=1, activation=\"logistic\", max_iter=1000, batch_size=10000, early_stopping=True),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors  :  0.9627181208053691  | time taken :  7.291905403137207  secs\n",
      "Decision Tree  :  0.9179194630872484  | time taken :  19.403695583343506  secs\n",
      "Random Forest  :  0.9633557046979866  | time taken :  12.060685873031616  secs\n",
      "Iteration 1, loss = 0.17342767\n",
      "Validation score: 0.964038\n",
      "Iteration 2, loss = 0.15619425\n",
      "Validation score: 0.964038\n",
      "Iteration 3, loss = 0.15602948\n",
      "Validation score: 0.964038\n",
      "Iteration 4, loss = 0.15573954\n",
      "Validation score: 0.964038\n",
      "Iteration 5, loss = 0.15573093\n",
      "Validation score: 0.964038\n",
      "Iteration 6, loss = 0.15572164\n",
      "Validation score: 0.964038\n",
      "Iteration 7, loss = 0.15559323\n",
      "Validation score: 0.964038\n",
      "Iteration 8, loss = 0.15568591\n",
      "Validation score: 0.964038\n",
      "Iteration 9, loss = 0.15585901\n",
      "Validation score: 0.964038\n",
      "Iteration 10, loss = 0.15578122\n",
      "Validation score: 0.964038\n",
      "Iteration 11, loss = 0.15570570\n",
      "Validation score: 0.964038\n",
      "Iteration 12, loss = 0.15609515\n",
      "Validation score: 0.964038\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "PerceptronCLF  :  0.9633221476510068  | time taken :  96.89375805854797  secs\n",
      "AdaBoost  :  0.9633221476510068  | time taken :  61.17801475524902  secs\n",
      "Naive Bayes  :  0.9210738255033557  | time taken :  0.8664782047271729  secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/discriminant_analysis.py:686: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA  :  0.878238255033557  | time taken :  4.843602418899536  secs\n"
     ]
    }
   ],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    start = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    end = time.time()\n",
    "    print(name, \" : \", clf.score(x_val, y_val), \" | time taken : \", end-start, \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CatBoostClassifier()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "model.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\n",
    "model.score(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
